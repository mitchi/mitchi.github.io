Mon expérience avec Apache Spark et Scala
Par Edmond La Chance

C’est une expérience plutôt positive. J’ai utilisé Scala pour implémenter TSPARK, un générateur de tests combinatoires qui contient des algorithmes distribués. 
En général, je pense avoir un style assez homme des cavernes en ce qui concerne Scala.
En gros, je code comme un vieux C++, mais je maitrise bien les patterns fonctionnels d’Apache Spark pour transformer des collections d’éléments.

Je n’ai pas de problème avec le Garbage Collector. J’ai analysé la vitesse de mes programmes plusieurs fois, et si mon programme a pris 50 secondes à exécuter, le garbage collector ne prend même pas 100 ms pour faire son travail, je trouve ça très correct.
Je considère même utiliser le Serial Collector afin d’utiliser ma mémoire de façon plus compacte.


Les points positifs : 

1. Le langage est magnifique. J’adore la syntaxe. Quand je lis le code, c’est drôle à dire mais je trouve ça tout simplement beau.
2. Librairie standard très bien faite. Et on a également accès aux librairies Java.
3. Garbage collector
4. Les outils développés pour Scala sont vraiment au top. IntelliJ de JetBrains est un excellent éditeur, et il vient avec un débogueur complet. 
5. SBT avec le plugin assembly permet de faire des .jar qui contiennent toutes les dépendances.
6. Travailler sur les collections est un vrai plaisir avec map, flatMap, filter. En plus, c’est assez facile de paralléliser le code qui travaille sur la collection. Il suffit d’ajouter .par après flatMap, ou alors d’utiliser Apache Spark : 


sc.makeRDD(collection); 
collection.flatMap( elem => elem *2 )


Le point négatif

Aucun support pour break/continue dans le langage. C’est assez désastreux pour le programmeur qui aime contrôler avec précision son flot d’exécution. C’est si naturel de voir des algorithmes comme une boucle infinie, que l’on peut quitter si et seulement si certaines conditions sont remplies. J’implémente le break en créant une fonction qui return. Franchement, ça fait du code assez laid.





